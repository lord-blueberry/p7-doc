\section{Reconstruction from under-sampled Measurements}\label{intro}
In Signal Processing, continuous signals are represented with discrete samples. A digital recording of music, an image of a tree, or the measured velocity of a particle, all are discrete samples of continuous signals. The Nyquist-Shannon sampling rate tells us how many samples are needed to fully represent a signal: A signal which contains at most frequency $f$ should be sampled with a frequency higher than $2f$. For example if we record a piece of music where the highest tone is at 20kHz, sampling rate should be more than 40kHz. Then, the music piece gets recorded above the Nyquist-Shannon sampling rate. It is fully sampled and there is exactly one continuous signal(with maximum frequency $f$) which fits the measurement.

The Nyquist-Shannon sampling rate is not always achievable: Samples may be expensive to acquire, get lost, or incomplete by the nature of the measurement instrument. In this case we are dealing with under-sampled measurements. Many possible signals fit the measurement and from the measurement alone, we cannot distinguish the true signal from all possibilities.

With the Theory of Compressed Sensing\cite{candes2006robust}\cite{donoho2006compressed} however, we can use prior information about the signal and find the most likely candidate from all possibilities. Under the right conditions, the most likely candidate is guaranteed to be the true signal. With the Theory of Compressed Sensing, we exploit prior information to reconstruct the true signal from under-sampled measurements.

In this project, the Theory of Compressed Sensing was applied to an image reconstruction problem of Radio Astronomy. Interferometers produce under-sampled measurements of the sky which have to be reconstructed by an algorithm. A Compressed Sensing approach was developed and implemented in the Common Astronomy Software Application (CASA). The reconstruction quality was compared to standard reconstruction algorithm in astronomy on VLA data of Supernova Remnant G55.


\subsection{Image Reconstruction for Radio Interferometers}
Radio Interferometers use several antennas spaced apart from each other. Each antenna pair measures the different arrival time of a radio wave. For small field of view imaging, each antenna pair measures approximately two dimensional Fourier components of the sky (called Visibility in Astronomy). An interferometer with 26 antennas measures 325 Visibilities. The distance between the antenna pair(called a baseline) dictates which Visibility of the image gets observed. The longer the baseline, the higher frequency of the observed Visibility. The longest baseline of an interferometer is therefore a rough estimate of its maximum resolution.

For wide field of view imaging, the two dimensional Fourier relation breaks apart as additional effects dominate the measurement. This project used small field of view imaging, and the two dimensional Fourier component is a good approximation. In this document, a measured Visibilitiy equals a two dimensional Fourier component.

The interferometer only samples as many Visibilities as it has antenna pairs. The task is to reconstruct the observed image from an under-sampled Fourier space. In Astronomy, the CLEAN class of algorithms\cite{hogbom1974aperture}\cite{schwab1984relaxing}\cite{rich2008multi}\cite{rau2011multi} were developed for this task. New interferometers like MeerKAT also observe new phenomenons which, in an ideal world, can be included in the CLEAN prior and improve its reconstruction. This is not possible in CLEAN, the prior is a fixed part of the algorithm. It cannot be modified as our prior knowledge of radio images increases.

Compressed Sensing can be thought of as a generalization of the CLEAN algorithm. The prior can be exchanged without changing the rest of the reconstruction algorithm. Furthermore Compressed Sensing reconstructions may be able to super-resolve the image\cite{girard2015sparse}.



\subsection{Deconvolution of the Dirty Image}
The ideal interferometer would fully sample the Fourier Space and introduce no instrumental effects and the observed image can be calculated with the inverse Fourier Transform. A real world interferometer corrupts the observed image with the image with under-sampling and other instrumental effects. The inverse Fourier Transform of the Visibilities can still be calculated, but it results in a corrupted "dirty" image. The observed image is convolved with a Point Spread Function (PSF) image with a Point Spread Function(PSF). The task is to reconstruct the observed image from the dirty image and the PSF, or more formally we try to solve for $x$ in equation \eqref{intro:eq:deconvolve}, where only the $PSF$ and $I_{dirty}$ are known($\star$ is used as the convolution operation).

\begin{equation}\label{intro:eq:deconvolve}
x \star  PSF + N = I_{dirty} 
\end{equation}

A deconvolution algorithm should find the observed image $x$. However, there may be many possible solutions to the equation \eqref{intro:eq:deconvolve}. Noise $N$ further complicates the deconvolution. The algorithm has to decide what is the most likely $x$ given $PSF$ and $I_{dirty}$. The PSF is specific to the interferometer. It models the sampling pattern in Fourier space. A deconvolution algorithm is not limited to a single interferometer and can easily be used on other instruments\footnote{This is true for small field of view imaging. Wide field of view introduces additional effects that typically do not get modelled with a PSF}.

Note that the deconvolution problem is just one way of formulating the reconstruction. It was used in this project since the CASA interface was built for deconvolution algorithms. The same problem can be formulated as in-painting the missing Visibilities: If we apply the Fourier Transform to equation \eqref{intro:eq:deconvolve}, then the convolution turns to a multiplication and we arrive at a similar equation $X*M + N = V$. The Fourier Transform of $I_{dirty}$ is just the measured Visibilities $V$,  $PSF$ turns into a masking matrix $M$. $M$ is one for all measured Visibilities and zero everywhere else. If an algorithm can in-paint the missing Visibilities $X$, it has also found a solution $x$ to the deconvolution problem of \eqref{intro:eq:deconvolve}. In section \ref{cs:objective} the different formulations get discussed in more detail.


\subsection{Deconvolution with CLEAN}
CLEAN assumes the image consists of single pixel wide point sources. This is true when the image contains stars, the majority of which are too far away from earth to have any extension. CLEAN therefore takes a dirty image and a PSF as input and tries to find point sources in the image. In each iteration of CLEAN, it searches the highest peak of the dirty image and removes a fraction of the PSF. It stops until the next highest peak is below a threshold, or if the maximum number of iterations was reached. The fraction of the PSF, threshold and number of iterations are tunable by the user. 

CLEAN optimizes the objective \eqref{intro:eq:clean} which is split in a data and a regularization term. The data term forces CLEAN to reconstruct close to the measurements, while the regularization term should account for noise. In each iteration, CLEAN searches the step which minimizes the objective the most, it uses a greedy optimization scheme. Due to the L0 "norm"\footnote{The L0 "norm" in this context is technically not a norm, hence the quotation marks. The L0 "norm" is a common notation in Compressed Sensing literature, therefore it is used here.}, the objective is non-convex and it may have local minima. Note that the L0 "norm" acts as the sum of non-zero elements in the image.

\begin{equation}\label{intro:eq:clean}
\underset{x}{minimize} \: \left \| I_{dirty} - x \star PSF \right \|_2^2 + \: \left \| x \right \|_0
\end{equation}

In practice, CLEAN gets stopped before it reaches a local minima. A common stopping criteria is if the next peak in the dirty image is below 3 to 5 times the estimated noise level. In that case, CLEAN detects all peaks that are higher than 3 to 5 times the estimated noise. 

CLEAN does a good approximation of the observed image, if it contains only point sources. Radio images may also contain extended emissions like hydrogen clouds, which are spread over several pixels. The maximum of extended emissions tends to be far lower than for point sources. Any pixel of the extended emission that is lower than 3 to 5 times the estimated noise gets ignored. If an extended emission is above the threshold, CLEAN approximates it with faint point sources. Instead of a hydrogen clouds, CLEAN may detect a cluster of faint stars.

To combat the problem, the reconstructed image of CLEAN gets convolved with the antenna beam-pattern. The beam-pattern is approximately a two dimensional Gaussian and represents the accuracy of the antennas. Clusters of faint stars get blurred into a more plausible cloud. This essentially lowers the resolution of the reconstructed image. The rationale is that any higher resolved structure is likely a reconstruction artefact. Over the years, the algorithm was extended, with the MS-MFS-CLEAN\cite{rau2011multi} as a recent example. All versions do blur the reconstructed image with the antenna beam-pattern.

The CLEAN prior works well for point sources, but it does not accurately reconstruct extended emissions. With the right prior super-resolved image reconstruction is possible. It was demonstrated in laboratory environment with the One Pixel Camera\cite{singlePixel}, or with the SASIR\cite{girard2015sparse} reconstruction algorithm for the LOFAR radio interferometer. Since the prior of CLEAN is a fixed part of the algorithm it cannot be modified without creating a new algorithm.


\subsection{CLEAN as Compressed Sensing Image Reconstruction}
Compressed Sensing Image Reconstruction is a generalization of the CLEAN algorithm. A Compressed Sensing Reconstruction algorithm consists of three separate parts:
\begin{itemize}
	\item An objective with a data and regularization term, and a parameter $\lambda$.
	\item A prior function $p()$.
	\item An optimization algorithm.
\end{itemize}

As the prior function $p()$, CLEAN uses the L0 "norm". The optimization algorithm is similar to Matching Pursuit. In each iteration, Matching Pursuit searches the element in the regularization term that minimizes the objective the most. The objective of CLEAN can also be generalized into \eqref{intro:eq:csclean}. The objective still has a data and regularization term, but now the parameter $\lambda$ represents the trade-off between reconstructing close to the measurement and reconstructing a plausible image.

\begin{equation}\label{intro:eq:csclean}
\underset{x}{minimize} \: \left \| I_{dirty} - x \star PSF \right \|_2^2 \: + \: \lambda \: p(x) 
\end{equation}

CLEAN is similar to a Compressed Sensing Reconstruction consisting of the L0 "norm", matching pursuit and the objective \eqref{intro:eq:csclean}. But now, individual parts of the Compressed Sensing Reconstruction can be replaced. For example instead of the L0 "norm", one could use the L2 norm, or apply the Haar Transform and use the L0 "norm" on the Haar Wavelet coefficients of $x$. The prior function $p()$ can be chosen according to our prior knowledge and potentially super-resolve the image $x$.



